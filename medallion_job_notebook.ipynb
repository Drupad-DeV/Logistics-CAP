{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "609fa605-3b86-46db-b12c-495d80717ce4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "bronze_path = \"/Volumes/logistics/bronze/shipments_vol/\"\n",
    "\n",
    "all_files_df = (\n",
    "    spark.createDataFrame(dbutils.fs.ls(bronze_path))\n",
    "    .filter(F.col(\"name\").endswith(\".csv\"))\n",
    "    .select(F.col(\"name\").alias(\"file_name\"))\n",
    ")\n",
    "all_files_df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8bfb24d7-bf67-4b58-8c4c-164cb204fc7a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "processed_df = spark.table(\"logistics.control.processed_files\")\n",
    "\n",
    "new_files_df = all_files_df.join(\n",
    "    processed_df,\n",
    "    on=\"file_name\",\n",
    "    how=\"left_anti\"\n",
    ")\n",
    "new_files_df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04cb0b62-83a1-46fc-9b9c-8ed7f0c0ef00",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "new_files = [row.file_name for row in new_files_df.collect()]\n",
    "\n",
    "for file_name in new_files:\n",
    "\n",
    "    file_path = f\"{bronze_path}/{file_name}\"\n",
    "\n",
    "    # Read CSV\n",
    "    df = (\n",
    "        spark.read\n",
    "            .format(\"csv\")\n",
    "            .option(\"header\", \"true\")\n",
    "            .load(file_path)\n",
    "    )\n",
    "\n",
    "    # Ingest metadata\n",
    "    df = (\n",
    "        df\n",
    "        .withColumn(\"ingest_ts\", F.current_timestamp())\n",
    "        .withColumn(\"ingest_date\", F.current_date())\n",
    "        .withColumn(\"source_file\", F.lit(file_name))\n",
    "    )\n",
    "\n",
    "    # Normalize columns\n",
    "    for c in df.columns:\n",
    "        df = df.withColumnRenamed(c, c.lower().replace(\" \", \"_\"))\n",
    "\n",
    "    # Trim + case\n",
    "    for c in [\"carrier_id\", \"delivery_status\"]:\n",
    "        df = df.withColumn(c, F.upper(F.trim(F.col(c))))\n",
    "\n",
    "    # Cast types\n",
    "    df = (\n",
    "        df\n",
    "        .withColumn(\"shipment_cost\", F.col(\"shipment_cost\").cast(\"double\"))\n",
    "        .withColumn(\"shipment_date\", F.to_date(\"shipment_date\"))\n",
    "        .withColumn(\"delivery_date\", F.to_date(\"delivery_date\"))\n",
    "    )\n",
    "\n",
    "    # Empty â†’ null\n",
    "    df = df.select([\n",
    "        F.when(F.trim(F.col(c)) == \"\", None).otherwise(F.col(c)).alias(c)\n",
    "        for c in df.columns\n",
    "    ])\n",
    "\n",
    "    # Dedup within file\n",
    "    w = Window.partitionBy(\"shipment_id\").orderBy(F.col(\"ingest_ts\").desc())\n",
    "    df = df.withColumn(\"rn\", F.row_number().over(w)).filter(\"rn=1\").drop(\"rn\")\n",
    "\n",
    "    # UNKNOWN carrier\n",
    "    df = df.withColumn(\"carrier_id\", F.coalesce(F.col(\"carrier_id\"), F.lit(\"UNKNOWN\")))\n",
    "\n",
    "    # Cost rounding\n",
    "    df = df.withColumn(\"shipment_cost\", F.round(F.col(\"shipment_cost\"), 2))\n",
    "\n",
    "    # Outlier detection\n",
    "    median_cost = df.approxQuantile(\"shipment_cost\", [0.5], 0.01)[0]\n",
    "    df = df.withColumn(\"dq_cost_outlier\", F.col(\"shipment_cost\") > median_cost * 10)\n",
    "\n",
    "    # DQ flags\n",
    "    df = (\n",
    "        df\n",
    "        .withColumn(\"dq_cost_null\", F.col(\"shipment_cost\").isNull())\n",
    "        .withColumn(\"dq_cost_negative\", F.col(\"shipment_cost\") < 0)\n",
    "        .withColumn(\"dq_cost_zero\", F.col(\"shipment_cost\") == 0)\n",
    "        .withColumn(\"dq_date_anomaly\", F.col(\"shipment_date\") > F.col(\"delivery_date\"))\n",
    "        .withColumn(\n",
    "            \"dq_invalid_status\",\n",
    "            ~F.col(\"delivery_status\").isin(\"DELIVERED\",\"IN_TRANSIT\",\"CANCELLED\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Score + severity\n",
    "    df = df.withColumn(\n",
    "        \"dq_score\",\n",
    "        F.expr(\"\"\"\n",
    "            int(dq_cost_null) +\n",
    "            int(dq_cost_negative) +\n",
    "            int(dq_cost_zero) +\n",
    "            int(dq_cost_outlier) +\n",
    "            int(dq_date_anomaly) +\n",
    "            int(dq_invalid_status)\n",
    "        \"\"\")\n",
    "    )\n",
    "\n",
    "    df = df.withColumn(\n",
    "        \"dq_severity\",\n",
    "        F.when(F.col(\"dq_score\") >= 2, \"CRITICAL\")\n",
    "             .when(F.col(\"dq_score\") == 1, \"MAJOR\")\n",
    "             .otherwise(\"MINOR\")\n",
    "    )\n",
    "\n",
    "    df = df.withColumn(\"is_quarantine\", F.col(\"dq_severity\") == \"CRITICAL\")\n",
    "\n",
    "    # Append to Silver\n",
    "    df.write \\\n",
    "      .format(\"delta\") \\\n",
    "      .mode(\"append\") \\\n",
    "      .saveAsTable(\"logistics.silver.shipments\")\n",
    "\n",
    "    # Mark file as processed\n",
    "    spark.createDataFrame(\n",
    "        [(file_name,)],\n",
    "        [\"file_name\"]\n",
    "    ).withColumn(\n",
    "        \"processed_ts\", F.current_timestamp()\n",
    "    ).write.mode(\"append\").saveAsTable(\"logistics.control.processed_files\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d76dabe-0d5d-4859-a3e9-e6f22952ffe6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT COUNT(*) AS total_records\n",
    "FROM logistics.silver.shipments;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c3f7bf2-be0e-48fb-a46c-40ca9f1df315",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT \n",
    "    COUNT(DISTINCT shipment_id) AS distinct_shipments\n",
    "FROM logistics.silver.shipments;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0067ddaf-17ff-41ff-b241-c86d94c71fbd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * FROM logistics.control.processed_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59c8c71a-a03f-43af-bba0-6f3211db3e94",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT \n",
    "    source_file,\n",
    "    COUNT(*) AS records_loaded\n",
    "FROM logistics.silver.shipments\n",
    "GROUP BY source_file\n",
    "ORDER BY source_file;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de024914-8f3a-4323-b28f-179a3d10cb38",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT COUNT(*) AS files_processed\n",
    "FROM logistics.control.processed_files;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e374e553-536b-4160-8938-df7e765a46f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT COUNT(*) AS null_cost_records\n",
    "FROM logistics.silver.shipments\n",
    "WHERE dq_cost_null = true;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a2b4f3f-5c56-4605-8ecc-2b3a4a4b611b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT COUNT(*) AS date_anomaly_records\n",
    "FROM logistics.silver.shipments\n",
    "WHERE dq_date_anomaly = true;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c52cf40-80c5-45c8-8b08-559f213abd52",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT shipment_id, shipment_date, delivery_date\n",
    "FROM logistics.silver.shipments\n",
    "WHERE dq_date_anomaly = true\n",
    "LIMIT 20;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e24fb0c8-0b42-4147-8685-68a686c7699c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT \n",
    "    dq_severity,\n",
    "    COUNT(*) AS record_count\n",
    "FROM logistics.silver.shipments\n",
    "GROUP BY dq_severity\n",
    "ORDER BY dq_severity;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ebe2e98a-1b7c-491b-911e-bf169cc3cc0a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT\n",
    "    COUNT(*) AS total_records,\n",
    "    SUM(CASE WHEN dq_severity = 'CRITICAL' THEN 1 ELSE 0 END) AS critical_records,\n",
    "    SUM(CASE WHEN dq_severity = 'MAJOR' THEN 1 ELSE 0 END) AS major_records,\n",
    "    SUM(CASE WHEN dq_severity = 'MINOR' THEN 1 ELSE 0 END) AS minor_records\n",
    "FROM logistics.silver.shipments;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95801b32-522c-4c8c-b516-3cc1f76f0add",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "shipments_df = spark.table(\"logistics.silver.shipments\")\n",
    "silver_cleaned_df = shipments_df.filter(col(\"dq_severity\") == \"MINOR\")\n",
    "quarantine_df = shipments_df.filter(\n",
    "    col(\"dq_severity\").isin(\"MAJOR\", \"CRITICAL\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f3a3973-6ce3-4858-9e6d-9dd3d2122999",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "silver_cleaned_df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .saveAsTable(\"logistics.silver.silver_cleaned\")\n",
    "quarantine_df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .saveAsTable(\"logistics.silver.quarantine\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "306bc6b9-b740-4575-97cb-027934a62afb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT COUNT(*) FROM logistics.silver.silver_cleaned;\n",
    "SELECT COUNT(*) FROM logistics.silver.quarantine;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f36e097-d3c8-4504-ba15-5c300fabbc79",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "silver_cleaned_df = spark.table(\"logistics.silver.silver_cleaned\")\n",
    "\n",
    "silver_cleaned_df \\\n",
    "    .coalesce(1) \\\n",
    "    .write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv(\"abfss://silver@ltimc1sacc.dfs.core.windows.net/silver_cleaned_csv/\")\n",
    "\n",
    "quarantine_df = spark.table(\"logistics.silver.quarantine\")\n",
    "\n",
    "quarantine_df \\\n",
    "    .coalesce(1) \\\n",
    "    .write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv(\"abfss://silver@ltimc1sacc.dfs.core.windows.net/quarantine_csv/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "183da2d0-0a19-492c-9815-bb83f52579df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Gold Layer**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "efca4481-a26f-496f-b65c-57921b1540b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "dim_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d4546db-0e46-4ba2-b7d8-b317b7e7f3b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE TABLE logistics.gold.dim_date AS\n",
    "SELECT\n",
    "  CAST(date_format(shipment_date, 'yyyyMMdd') AS INT) AS date_sk,\n",
    "  shipment_date AS full_date,\n",
    "  year(shipment_date) AS year,\n",
    "  month(shipment_date) AS month,\n",
    "  day(shipment_date) AS day\n",
    "FROM (\n",
    "  SELECT DISTINCT shipment_date\n",
    "  FROM logistics.silver.silver_cleaned\n",
    ");\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "554da183-cb7f-4fd1-851a-cd1b213f5f24",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM logistics.gold.dim_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ecb5db3-b1a0-41a1-8000-198f9fc01dcf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "regions_df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(\"/Volumes/logistics/gold/dimtables/regions.csv\")\n",
    "\n",
    "regions_df.createOrReplaceTempView(\"regions_stg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4f999c2-0a97-4d33-9a8f-a24384f3ed48",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE TABLE logistics.gold.dim_region AS\n",
    "SELECT\n",
    "  row_number() OVER (ORDER BY region_id) AS region_sk,\n",
    "  region_id,\n",
    "  region_name\n",
    "FROM regions_stg;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40349f91-ff52-426b-8b73-924c15ad9438",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM logistics.gold.dim_region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7de225b-c678-4b6b-b98b-a926a3433304",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "warehouses_df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(\"/Volumes/logistics/gold/dimtables/warehouses.csv\")\n",
    "\n",
    "warehouses_df.createOrReplaceTempView(\"warehouses_stg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90ac7adf-ac5d-4291-8349-bdd87e65f776",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE TABLE logistics.gold.dim_warehouse AS\n",
    "SELECT\n",
    "  row_number() OVER (ORDER BY warehouse_id) AS warehouse_sk,\n",
    "  warehouse_id,\n",
    "  warehouse_name,\n",
    "  city,\n",
    "  capacity_tpd\n",
    "FROM warehouses_stg;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2ae79ff-cef4-45ae-82b6-74c7c3d26ec0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM logistics.gold.dim_warehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ccb0bb89-1b30-421f-870f-16b193373e8f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "carriers_df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(\"/Volumes/logistics/gold/dimtables/carriers.csv\")\n",
    "\n",
    "carriers_df.createOrReplaceTempView(\"carriers_stg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "149dca1d-05dd-4788-b634-72cb7c9c1c48",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE TABLE logistics.gold.dim_carrier AS\n",
    "SELECT\n",
    "  row_number() OVER (ORDER BY carrier_id) AS carrier_sk,\n",
    "  carrier_id,\n",
    "  carrier_name,\n",
    "  mode\n",
    "FROM carriers_stg;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a73e1a3-cf43-43fd-a8ea-0b95069dc824",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM logistics.gold.dim_carrier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02ae2505-c774-4b07-9bf1-ac27ecc499aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE TABLE logistics.gold.fact_shipments AS\n",
    "SELECT\n",
    "    s.shipment_id,\n",
    "\n",
    "    COALESCE(dc.carrier_sk, -1) AS carrier_sk,\n",
    "    dw.warehouse_sk,\n",
    "    dr.region_sk,\n",
    "    dd.date_sk,\n",
    "\n",
    "    s.shipment_cost,\n",
    "    s.delivery_days,\n",
    "    s.priority_level,\n",
    "    s.is_fragile,\n",
    "    s.delivery_status,\n",
    "    s.payment_type\n",
    "\n",
    "FROM logistics.silver.silver_cleaned s\n",
    "\n",
    "LEFT JOIN logistics.gold.dim_carrier dc\n",
    "    ON s.carrier_id = dc.carrier_id\n",
    "\n",
    "LEFT JOIN logistics.gold.dim_warehouse dw\n",
    "    ON s.warehouse_id = dw.warehouse_id\n",
    "\n",
    "LEFT JOIN logistics.gold.dim_region dr\n",
    "    ON s.region_id = dr.region_id\n",
    "\n",
    "LEFT JOIN logistics.gold.dim_date dd\n",
    "    ON s.shipment_date = dd.full_date\n",
    "\n",
    "WHERE s.shipment_cost IS NOT NULL;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e082321-82f1-45f5-849f-5fd8033de101",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM logistics.gold.fact_shipments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83f72464-5200-4773-8d51-44cf720b3788",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT\n",
    "    COUNT(*) AS total_rows,\n",
    "    SUM(CASE WHEN carrier_sk IS NULL THEN 1 ELSE 0 END) AS null_carrier,\n",
    "    SUM(CASE WHEN warehouse_sk IS NULL THEN 1 ELSE 0 END) AS null_warehouse,\n",
    "    SUM(CASE WHEN region_sk IS NULL THEN 1 ELSE 0 END) AS null_region\n",
    "FROM logistics.gold.fact_shipments;\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8690973069196468,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Cleaning_final_silver",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}